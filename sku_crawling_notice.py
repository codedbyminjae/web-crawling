import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urlparse, parse_qs
import re

# 1) PK ì¶”ì¶œ í•¨ìˆ˜
def extract_post_id(url):
    parsed = urlparse(url)
    query = parse_qs(parsed.query)

    # index.php?document_srl=12353
    if "document_srl" in query:
        return query["document_srl"][0]

    # /12353
    tail = parsed.path.split("/")[-1]
    if tail.isdigit():
        return tail

    return None

# 2) ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
def crawl_detail(url):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")

    # ì œëª©
    title = soup.select_one(
        "#content > div.fbox.content-box.notice > article > div > div.viewDocument > div > div.boardReadHeader > div.titleArea > h3 > a"
    ).get_text(strip=True)

    # ì‘ì„±ì
    writer = soup.select_one(
        "#content > div.fbox.content-box.notice > article > div > div.viewDocument > div > div.boardReadHeader > div.authorArea > a"
    ).get_text(strip=True)

    # ë‚ ì§œ (ì‹œê°„ ì œê±°)
    date = soup.select_one(
        "#content > div.fbox.content-box.notice > article > div > div.viewDocument > div > div.boardReadHeader > div.titleArea > span > span.date"
    ).get_text(strip=True)
    date = date.split()[0]  # â† ë‚ ì§œë§Œ ë‚¨ê¸°ê¸°

    # ë³¸ë¬¸ (ì¤„ë°”ê¿ˆ ì œê±° + í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬)
    body_div = soup.select_one(
        "#content > div.fbox.content-box.notice > article > div > div.viewDocument > div > div.boardReadBody > div"
    )

    if body_div:
        raw = body_div.get_text(" ", strip=True)
        body = re.sub(r"\s+", " ", raw)  # ê³µë°± ì—¬ëŸ¬ ê°œ â†’ í•˜ë‚˜ë¡œ
    else:
        body = ""

    return title, body, writer, date

# 3) í˜ì´ì§€ ìë™ ì´ë™í•˜ë©° 2021ë…„ê¹Œì§€ ìˆ˜ì§‘
BASE_URL = "https://cs.skuniv.ac.kr/index.php?mid=cs_notice&page="
page = 0

seen_ids = set()
results = []

while True:
    print(f"\ní˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")

    url = BASE_URL + str(page)
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")

    raw_links = soup.select("td.title a")

    if not raw_links:
        print("ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŒ â†’ ì¢…ë£Œ")
        break

    stop_flag = False  # 2021ë…„ ì´ì „ì´ë©´ ì¢…ë£Œ

    for a in raw_links:
        href = a.get("href", "")

        # ìŠ¤íŒ¸ ë§í¬ ì œê±°
        if not href or href.startswith("#"):
            continue

        # ê¸€ ë§í¬ í•„í„°ë§
        if (href.startswith("/") and href[1:].isdigit()) or ("document_srl=" in href):

            detail_url = "https://cs.skuniv.ac.kr" + href if href.startswith("/") else href
            post_id = extract_post_id(detail_url)

            if not post_id:
                continue

            # ì¤‘ë³µ ì œê±°
            if post_id in seen_ids:
                continue
            seen_ids.add(post_id)

            # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
            print(f"- í¬ë¡¤ë§ ì¤‘: {detail_url}")
            title, body, writer, date = crawl_detail(detail_url)

            # ğŸ›‘ 2021ë…„ ì´ì „ ê²Œì‹œê¸€ì´ë©´ STOP
            if (date.startswith("2020") or
                date.startswith("2019") or
                date.startswith("2018") or
                date.startswith("2017") or
                date.startswith("2016")):
                print("ğŸ›‘ 2021ë…„ ì´ì „ ê²Œì‹œê¸€ ë„ë‹¬ â†’ ì¢…ë£Œ")
                stop_flag = True
                break

            results.append([post_id, title, body, writer, date, "ê³µì§€ì‚¬í•­"])

    if stop_flag:
        break

    page += 1

# 4) CSV ì €ì¥
with open("notice_until_2021.csv", "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["id", "title", "body", "writer", "date", "type"])
    writer.writerows(results)

print("\ní¬ë¡¤ë§ ì™„ë£Œ. notice_until_2021.csv ìƒì„±ë¨.")
print(f"ì´ {len(results)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¨.")
